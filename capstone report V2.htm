





<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://assets-cdn.github.com">
  <link rel="dns-prefetch" href="https://avatars0.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars1.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars2.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars3.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">



  <link crossorigin="anonymous" media="all" integrity="sha512-lLo2nlsdl+bHLu6PGvC2j3wfP45RnK4wKQLiPnCDcuXfU38AiD+JCdMywnF3WbJC1jaxe3lAI6AM4uJuMFBLEw==" rel="stylesheet" href="https://assets-cdn.github.com/assets/frameworks-08fc49d3bd2694c870ea23d0906f3610.css" />
  <link crossorigin="anonymous" media="all" integrity="sha512-4kfWSrzu4OShEnC5m0lqUCfKkZfG7JH0ff4wnEtubTUTZqV5pS5oUMTOvWE2DDL7ttjZ9FpnZInl/0TLO3EIiA==" rel="stylesheet" href="https://assets-cdn.github.com/assets/github-6c1d4c04bb55a87b9cb81ffdbd683662.css" />
  
  
  
  
  

  <meta name="viewport" content="width=device-width">
  
  <title>Capstone---NLP-project/capstone report template.md at master · alstampe/Capstone---NLP-project</title>
    <meta name="description" content="Capstone project - NLP for text analysis. Contribute to alstampe/Capstone---NLP-project development by creating an account on GitHub.">
    <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
  <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
  <meta property="fb:app_id" content="1401488693436528">

    
    <meta property="og:image" content="https://avatars1.githubusercontent.com/u/42574791?s=400&amp;v=4" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="alstampe/Capstone---NLP-project" /><meta property="og:url" content="https://github.com/alstampe/Capstone---NLP-project" /><meta property="og:description" content="Capstone project - NLP for text analysis. Contribute to alstampe/Capstone---NLP-project development by creating an account on GitHub." />

  <link rel="assets" href="https://assets-cdn.github.com/">
  <link rel="web-socket" href="wss://live.github.com/_sockets/VjI6MzI3NjkwNDQyOjRkZDY4MDIxNWU2ZDllNzcyMjMwNTNhNWFmMDYzMWQ4OTllNjRmMDE4ZDg3MmM5M2Y2YTUzODJmMTExNzJjZjk=--c7e08f5b08f1c07b9719c31f181697a1c319bb71">
  <meta name="pjax-timeout" content="1000">
  <link rel="sudo-modal" href="/sessions/sudo_modal">
  <meta name="request-id" content="E46A:567A:129F97F:2223AE5:5C03B45B" data-pjax-transient>


  

  <meta name="selected-link" value="repo_source" data-pjax-transient>

      <meta name="google-site-verification" content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU">
    <meta name="google-site-verification" content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA">
    <meta name="google-site-verification" content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc">

  <meta name="octolytics-host" content="collector.githubapp.com" /><meta name="octolytics-app-id" content="github" /><meta name="octolytics-event-url" content="https://collector.githubapp.com/github-external/browser_event" /><meta name="octolytics-dimension-request_id" content="E46A:567A:129F97F:2223AE5:5C03B45B" /><meta name="octolytics-dimension-region_edge" content="iad" /><meta name="octolytics-dimension-region_render" content="iad" /><meta name="octolytics-actor-id" content="42574791" /><meta name="octolytics-actor-login" content="alstampe" /><meta name="octolytics-actor-hash" content="865a8bcf0a56eb7624836f3635eb7d0af5215c627cb25ea1a79db388e55fde26" />
<meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show" data-pjax-transient="true" />



    <meta name="google-analytics" content="UA-3769691-2">

  <meta class="js-ga-set" name="userId" content="ef742860b4a7a63dd66040f22c512311" %>

<meta class="js-ga-set" name="dimension1" content="Logged In">



  

      <meta name="hostname" content="github.com">
    <meta name="user-login" content="alstampe">

      <meta name="expected-hostname" content="github.com">
    <meta name="js-proxy-site-detection-payload" content="OWE2OWVhNzExNjY4MjJjNDQyOTUyYWQxNWI5YzAyMTBjNGY1NjBhNWRjZDgzOWZhMjI1MjMxMmUyMTBiZDQ2NXx7InJlbW90ZV9hZGRyZXNzIjoiODAuMjEzLjg0LjE4MiIsInJlcXVlc3RfaWQiOiJFNDZBOjU2N0E6MTI5Rjk3RjoyMjIzQUU1OjVDMDNCNDVCIiwidGltZXN0YW1wIjoxNTQzNzQ2NjY2LCJob3N0IjoiZ2l0aHViLmNvbSJ9">

    <meta name="enabled-features" content="DASHBOARD_V2_LAYOUT_OPT_IN,EXPLORE_DISCOVER_REPOSITORIES,UNIVERSE_BANNER,MARKETPLACE_PLAN_RESTRICTION_EDITOR,NOTIFY_ON_BLOCK,TIMELINE_COMMENT_UPDATES,RELATED_ISSUES,MARKETPLACE_INSIGHTS_V2">

  <meta name="html-safe-nonce" content="a8de8510321a4c57cbd7861c3674c13e088b97e0">

  <meta http-equiv="x-pjax-version" content="cdc9a5bb30661c6c1c8e99ce4d3646f2">
  

      <link href="https://github.com/alstampe/Capstone---NLP-project/commits/master.atom" rel="alternate" title="Recent Commits to Capstone---NLP-project:master" type="application/atom+xml">

  <meta name="go-import" content="github.com/alstampe/Capstone---NLP-project git https://github.com/alstampe/Capstone---NLP-project.git">

  <meta name="octolytics-dimension-user_id" content="42574791" /><meta name="octolytics-dimension-user_login" content="alstampe" /><meta name="octolytics-dimension-repository_id" content="157568167" /><meta name="octolytics-dimension-repository_nwo" content="alstampe/Capstone---NLP-project" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="157568167" /><meta name="octolytics-dimension-repository_network_root_nwo" content="alstampe/Capstone---NLP-project" /><meta name="octolytics-dimension-repository_explore_github_marketplace_ci_cta_shown" content="true" />


    <link rel="canonical" href="https://github.com/alstampe/Capstone---NLP-project/blob/master/capstone%20report%20template.md" data-pjax-transient>


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <link rel="mask-icon" href="https://assets-cdn.github.com/pinned-octocat.svg" color="#000000">
  <link rel="icon" type="image/x-icon" class="js-site-favicon" href="https://assets-cdn.github.com/favicon.ico">

<meta name="theme-color" content="#1e2327">

  <meta name="msapplication-TileImage" content="/windows-tile.png">
  <meta name="msapplication-TileColor" content="#ffffff">


  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-in env-production page-blob">
    

  <div class="position-relative js-header-wrapper ">
    <a href="#start-of-content" tabindex="1" class="p-3 bg-blue text-white show-on-focus js-skip-to-content">Skip to content</a>
    <div id="js-pjax-loader-bar" class="pjax-loader-bar"><div class="progress"></div></div>

    
    
    


        
<header class="Header  f5" role="banner">
  <div class="d-flex flex-justify-between px-3 ">
    <div class="d-flex flex-justify-between ">
      <div class="">
        <a class="header-logo-invertocat" href="https://github.com/" data-hotkey="g d" aria-label="Homepage" data-ga-click="Header, go to dashboard, icon:logo">
  <svg height="32" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
</a>

      </div>

    </div>

    <div class="HeaderMenu d-flex flex-justify-between flex-auto">
      <nav class="d-flex" aria-label="Global">
            <div class="">
              <div class="header-search scoped-search site-scoped-search js-site-search position-relative js-jump-to"
  role="combobox"
  aria-owns="jump-to-results"
  aria-label="Search or jump to"
  aria-haspopup="listbox"
  aria-expanded="false"
>
  <div class="position-relative">
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-site-search-form" data-scope-type="Repository" data-scope-id="157568167" data-scoped-search-url="/alstampe/Capstone---NLP-project/search" data-unscoped-search-url="/search" action="/alstampe/Capstone---NLP-project/search" accept-charset="UTF-8" method="get"><input name="utf8" type="hidden" value="&#x2713;" />
      <label class="form-control header-search-wrapper header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center js-chromeless-input-container">
        <input type="text"
          class="form-control header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable"
          data-hotkey="s,/"
          name="q"
          value=""
          placeholder="Search or jump to…"
          data-unscoped-placeholder="Search or jump to…"
          data-scoped-placeholder="Search or jump to…"
          autocapitalize="off"
          aria-autocomplete="list"
          aria-controls="jump-to-results"
          aria-label="Search or jump to…"
          data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations#csrf-token=m2G8UCT72gfBdiHKtZxCvLUZbl689lUSxarAHFiM3VvZvavXBe+kTikcXNdN1EFNzvIDZvTPuvNh3DnhWW75Hg=="
          spellcheck="false"
          autocomplete="off"
          >
          <input type="hidden" class="js-site-search-type-field" name="type" >
            <img src="https://assets-cdn.github.com/images/search-key-slash.svg" alt="" class="mr-2 header-search-key-slash">

            <div class="Box position-absolute overflow-hidden d-none jump-to-suggestions js-jump-to-suggestions-container">
              
<ul class="d-none js-jump-to-suggestions-template-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-suggestion" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

</ul>

<ul class="d-none js-jump-to-no-results-template-container">
  <li class="d-flex flex-justify-center flex-items-center f5 d-none js-jump-to-suggestion p-2">
    <span class="text-gray">No suggested jump to results</span>
  </li>
</ul>

<ul id="jump-to-results" role="listbox" class="p-0 m-0 js-navigation-container jump-to-suggestions-results-container js-jump-to-suggestions-results-container">
  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-scoped-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>

  

<li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-global-search d-none" role="option">
  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2" href="">
    <div class="jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none">
      <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"/></svg>
    </div>

    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none" alt="" aria-label="Team" src="" width="28" height="28">

    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
    </div>

    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
        In this repository
      </span>
      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
        All GitHub
      </span>
      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>

    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
      Jump to
      <span class="d-inline-block ml-1 v-align-middle">↵</span>
    </div>
  </a>
</li>


    <li class="d-flex flex-justify-center flex-items-center p-0 f5 js-jump-to-suggestion">
      <img src="https://assets-cdn.github.com/images/spinners/octocat-spinner-128.gif" alt="Octocat Spinner Icon" class="m-2" width="28">
    </li>
</ul>

            </div>
      </label>
</form>  </div>
</div>

            </div>

          <ul class="d-flex pl-2 flex-items-center text-bold list-style-none">
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g p" data-ga-click="Header, click, Nav menu - item:pulls context:user" aria-label="Pull requests you created" data-selected-links="/pulls /pulls/assigned /pulls/mentioned /pulls" href="/pulls">
                Pull requests
</a>            </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g i" data-ga-click="Header, click, Nav menu - item:issues context:user" aria-label="Issues you created" data-selected-links="/issues /issues/assigned /issues/mentioned /issues" href="/issues">
                Issues
</a>            </li>
              <li class="position-relative">
                <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:marketplace context:user" data-octo-click="marketplace_click" data-octo-dimensions="location:nav_bar" data-selected-links=" /marketplace" href="/marketplace">
                   Marketplace
</a>                  
              </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:explore" data-selected-links="/explore /trending /trending/developers /integrations /integrations/feature/code /integrations/feature/collaborate /integrations/feature/ship showcases showcases_search showcases_landing /explore" href="/explore">
                Explore
</a>            </li>
          </ul>
      </nav>

      <div class="d-flex">
        
<ul class="user-nav d-flex flex-items-center list-style-none" id="user-links">
  <li class="dropdown">
    <span class="d-inline-block  px-2">
      
    <a aria-label="You have no unread notifications" class="notification-indicator tooltipped tooltipped-s  js-socket-channel js-notification-indicator" data-hotkey="g n" data-ga-click="Header, go to notifications, icon:read" data-channel="notification-changed:42574791" href="/notifications">
        <span class="mail-status "></span>
        <svg class="octicon octicon-bell" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 12v1H0v-1l.73-.58c.77-.77.81-2.55 1.19-4.42C2.69 3.23 6 2 6 2c0-.55.45-1 1-1s1 .45 1 1c0 0 3.39 1.23 4.16 5 .38 1.88.42 3.66 1.19 4.42l.66.58H14zm-7 4c1.11 0 2-.89 2-2H5c0 1.11.89 2 2 2z"/></svg>
</a>
    </span>
  </li>

  <li class="dropdown">
    <details class="details-overlay details-reset d-flex px-2 flex-items-center">
      <summary class="HeaderNavlink"
         aria-label="Create new…"
         data-ga-click="Header, create new, icon:add">
        <svg class="octicon octicon-plus float-left mr-1 mt-1" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 9H7v5H5V9H0V7h5V2h2v5h5v2z"/></svg>
        <span class="dropdown-caret mt-1"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw">
        
<a role="menuitem" class="dropdown-item" href="/new" data-ga-click="Header, create new repository">
  New repository
</a>

  <a role="menuitem" class="dropdown-item" href="/new/import" data-ga-click="Header, import a repository">
    Import repository
  </a>

<a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, create new gist">
  New gist
</a>

  <a role="menuitem" class="dropdown-item" href="/organizations/new" data-ga-click="Header, create new organization">
    New organization
  </a>


  <div class="dropdown-divider"></div>
  <div class="dropdown-header">
    <span title="alstampe/Capstone---NLP-project">This repository</span>
  </div>
    <a role="menuitem" class="dropdown-item" href="/alstampe/Capstone---NLP-project/issues/new" data-ga-click="Header, create new issue">
      New issue
    </a>


      </details-menu>
    </details>
  </li>

  <li class="dropdown">

    <details class="details-overlay details-reset d-flex pl-2 flex-items-center">
      <summary class="HeaderNavlink name mt-1"
        aria-label="View profile and more"
        data-ga-click="Header, show menu, icon:avatar">
        <img alt="@alstampe" class="avatar float-left mr-1" src="https://avatars0.githubusercontent.com/u/42574791?s=40&amp;v=4" height="20" width="20">
        <span class="dropdown-caret"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw">
        <ul>
          <li class="header-nav-current-user css-truncate"><a role="menuitem" class="no-underline user-profile-link px-3 pt-2 pb-2 mb-n2 mt-n1 d-block" href="/alstampe" data-ga-click="Header, go to profile, text:Signed in as">Signed in as <strong class="css-truncate-target">alstampe</strong></a></li>
          <li class="dropdown-divider"></li>
          <li><a role="menuitem" class="dropdown-item" href="/alstampe" data-ga-click="Header, go to profile, text:your profile">Your profile</a></li>
          <li><a role="menuitem" class="dropdown-item" href="/alstampe?tab=repositories" data-ga-click="Header, go to repositories, text:your repositories">Your repositories</a></li>


          <li><a role="menuitem" class="dropdown-item" href="/alstampe?tab=stars" data-ga-click="Header, go to starred repos, text:your stars">Your stars</a></li>
            <li><a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, your gists, text:your gists">Your gists</a></li>
          <li class="dropdown-divider"></li>
          <li><a role="menuitem" class="dropdown-item" href="https://help.github.com" data-ga-click="Header, go to help, text:help">Help</a></li>
          <li><a role="menuitem" class="dropdown-item" href="/settings/profile" data-ga-click="Header, go to settings, icon:settings">Settings</a></li>
          <li>
            <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="logout-form" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="+10w+0ovSvkDcusSOmQfj4HhYQ/eaf5lGxQPLIJRxM/0zcmWBkpX3VZ+AL2KITWDhnO1ZqDzrerCRBg8jkTutg==" />
              <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout" role="menuitem">
                Sign out
              </button>
</form>          </li>
        </ul>
      </details-menu>
    </details>
  </li>
</ul>



        <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="sr-only right-0" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="M2yq0tY4eu012GNWXx+FmSJp7LhvEp8OKsE/oY2Mktg8/FO/ml1nyWDUiPnvWq+VJfs40RGIzIHzkSixgZm4oQ==" />
          <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout">
            Sign out
          </button>
</form>      </div>
    </div>
  </div>
</header>

      

  </div>

  <div id="start-of-content" class="show-on-focus"></div>

    <div id="js-flash-container">


</div>



  <div role="main" class="application-main " >
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <div id="js-repo-pjax-container" data-pjax-container >
      







  <div class="pagehead repohead instapaper_ignore readability-menu experiment-repo-nav  ">
    <div class="repohead-details-container clearfix container">

      <ul class="pagehead-actions">
  <li>
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form data-remote="true" class="js-social-form js-social-container" action="/notifications/subscribe" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="7AIAJJKsxXRHPN5arZSlWN5JEWSFoEKQ65egsBrFjybcOXD8xOe0WtrIi8jgSZx+BhcobQtzo8N11WYZKxjKDQ==" />      <input type="hidden" name="repository_id" id="repository_id" value="157568167" class="form-control" />

      <details class="details-reset details-overlay select-menu float-left">
        <summary class="btn btn-sm btn-with-count select-menu-button" data-ga-click="Repository, click Watch settings, action:blob#show">
          <span data-menu-button>
              <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
              Watch
          </span>
        </summary>
        <details-menu class="select-menu-modal position-absolute mt-5" style="z-index: 99;">
          <div class="select-menu-header">
            <span class="select-menu-title">Notifications</span>
          </div>
          <div class="select-menu-list">
            <button type="submit" name="do" value="included" class="select-menu-item width-full" aria-checked="true" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Not watching</span>
                <span class="description">Be notified only when participating or @mentioned.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                  Watch
                </span>
              </div>
            </button>

              <button type="submit" name="do" value="release_only" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
                <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
                <div class="select-menu-item-text">
                  <span class="select-menu-item-heading">Releases only</span>
                  <span class="description">Be notified of new releases, and when participating or @mentioned.</span>
                  <span class="hidden-select-button-text" data-menu-button-contents>
                    <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                    Unwatch releases
                  </span>
                </div>
              </button>

            <button type="submit" name="do" value="subscribed" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Watching</span>
                <span class="description">Be notified of all conversations.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
                  Unwatch
                </span>
              </div>
            </button>

            <button type="submit" name="do" value="ignore" class="select-menu-item width-full" aria-checked="false" role="menuitemradio">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Ignoring</span>
                <span class="description">Never be notified.</span>
                <span class="hidden-select-button-text" data-menu-button-contents>
                  <svg class="octicon octicon-mute v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 2.81v10.38c0 .67-.81 1-1.28.53L3 10H1c-.55 0-1-.45-1-1V7c0-.55.45-1 1-1h2l3.72-3.72C7.19 1.81 8 2.14 8 2.81zm7.53 3.22l-1.06-1.06-1.97 1.97-1.97-1.97-1.06 1.06L11.44 8 9.47 9.97l1.06 1.06 1.97-1.97 1.97 1.97 1.06-1.06L13.56 8l1.97-1.97z"/></svg>
                  Stop ignoring
                </span>
              </div>
            </button>
          </div>
        </details-menu>
      </details>
      <a class="social-count js-social-count"
        href="/alstampe/Capstone---NLP-project/watchers"
        aria-label="0 users are watching this repository">
        0
      </a>
</form>
  </li>

  <li>
    
  <div class="js-toggler-container js-social-container starring-container ">
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="starred js-social-form" action="/alstampe/Capstone---NLP-project/unstar" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="cLfas3L1HxJ7j0aEFQDWqPpLYts8ERiaoaBLNrohVlplGCnDScaayzwSVjUpSgixrE0IYvkXl0SOyHT7OWqcrg==" />
      <input type="hidden" name="context" value="repository"></input>
      <button
        type="submit"
        class="btn btn-sm btn-with-count js-toggler-target"
        aria-label="Unstar this repository" title="Unstar alstampe/Capstone---NLP-project"
        data-ga-click="Repository, click unstar button, action:blob#show; text:Unstar">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"/></svg>
        Unstar
      </button>
        <a class="social-count js-social-count" href="/alstampe/Capstone---NLP-project/stargazers"
           aria-label="0 users starred this repository">
          0
        </a>
</form>
    <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="unstarred js-social-form" action="/alstampe/Capstone---NLP-project/star" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="BpFhygPcY0DbKarAOr5vWmp3OdLV3Fw5NpL/ummpGpOxh17Eu5iBmxgoMeDYBwSgLcjJmKTV3VwXKy3xRMtyqw==" />
      <input type="hidden" name="context" value="repository"></input>
      <button
        type="submit"
        class="btn btn-sm btn-with-count js-toggler-target"
        aria-label="Star this repository" title="Star alstampe/Capstone---NLP-project"
        data-ga-click="Repository, click star button, action:blob#show; text:Star">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"/></svg>
        Star
      </button>
        <a class="social-count js-social-count" href="/alstampe/Capstone---NLP-project/stargazers"
           aria-label="0 users starred this repository">
          0
        </a>
</form>  </div>

  </li>

  <li>
        <span class="btn btn-sm btn-with-count disabled tooltipped tooltipped-sw" aria-label="Cannot fork because you own this repository and are not a member of any organizations.">
          <svg class="octicon octicon-repo-forked v-align-text-bottom" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1a1.993 1.993 0 0 0-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 0 0 2 1a1.993 1.993 0 0 0-1 3.72V6.5l3 3v1.78A1.993 1.993 0 0 0 5 15a1.993 1.993 0 0 0 1-3.72V9.5l3-3V4.72A1.993 1.993 0 0 0 8 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
          Fork
</span>
    <a href="/alstampe/Capstone---NLP-project/network/members" class="social-count"
       aria-label="0 users forked this repository">
      0
    </a>
  </li>
</ul>

      <h1 class="public ">
  <svg class="octicon octicon-repo" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
  <span class="author" itemprop="author"><a class="url fn" rel="author" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42574791" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/alstampe">alstampe</a></span><!--
--><span class="path-divider">/</span><!--
--><strong itemprop="name"><a data-pjax="#js-repo-pjax-container" href="/alstampe/Capstone---NLP-project">Capstone---NLP-project</a></strong>

</h1>

    </div>
    
<nav class="reponav js-repo-nav js-sidenav-container-pjax container"
     itemscope
     itemtype="http://schema.org/BreadcrumbList"
    aria-label="Repository"
     data-pjax="#js-repo-pjax-container">

  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a class="js-selected-navigation-item selected reponav-item" itemprop="url" data-hotkey="g c" aria-current="page" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages /alstampe/Capstone---NLP-project" href="/alstampe/Capstone---NLP-project">
      <svg class="octicon octicon-code" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M9.5 3L8 4.5 11.5 8 8 11.5 9.5 13 14 8 9.5 3zm-5 0L0 8l4.5 5L6 11.5 2.5 8 6 4.5 4.5 3z"/></svg>
      <span itemprop="name">Code</span>
      <meta itemprop="position" content="1">
</a>  </span>

    <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
      <a itemprop="url" data-hotkey="g i" class="js-selected-navigation-item reponav-item" data-selected-links="repo_issues repo_labels repo_milestones /alstampe/Capstone---NLP-project/issues" href="/alstampe/Capstone---NLP-project/issues">
        <svg class="octicon octicon-issue-opened" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg>
        <span itemprop="name">Issues</span>
        <span class="Counter">0</span>
        <meta itemprop="position" content="2">
</a>    </span>

  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a data-hotkey="g p" itemprop="url" class="js-selected-navigation-item reponav-item" data-selected-links="repo_pulls checks /alstampe/Capstone---NLP-project/pulls" href="/alstampe/Capstone---NLP-project/pulls">
      <svg class="octicon octicon-git-pull-request" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
      <span itemprop="name">Pull requests</span>
      <span class="Counter">0</span>
      <meta itemprop="position" content="3">
</a>  </span>


    <a data-hotkey="g b" class="js-selected-navigation-item reponav-item" data-selected-links="repo_projects new_repo_project repo_project /alstampe/Capstone---NLP-project/projects" href="/alstampe/Capstone---NLP-project/projects">
      <svg class="octicon octicon-project" viewBox="0 0 15 16" version="1.1" width="15" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      Projects
      <span class="Counter" >0</span>
</a>

    <a class="js-selected-navigation-item reponav-item" data-hotkey="g w" data-selected-links="repo_wiki /alstampe/Capstone---NLP-project/wiki" href="/alstampe/Capstone---NLP-project/wiki">
      <svg class="octicon octicon-book" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M3 5h4v1H3V5zm0 3h4V7H3v1zm0 2h4V9H3v1zm11-5h-4v1h4V5zm0 2h-4v1h4V7zm0 2h-4v1h4V9zm2-6v9c0 .55-.45 1-1 1H9.5l-1 1-1-1H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h5.5l1 1 1-1H15c.55 0 1 .45 1 1zm-8 .5L7.5 3H2v9h6V3.5zm7-.5H9.5l-.5.5V12h6V3z"/></svg>
      Wiki
</a>
  <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_graphs repo_contributors dependency_graph pulse alerts security /alstampe/Capstone---NLP-project/pulse" href="/alstampe/Capstone---NLP-project/pulse">
    <svg class="octicon octicon-graph" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 14v1H0V0h1v14h15zM5 13H3V8h2v5zm4 0H7V3h2v10zm4 0h-2V6h2v7z"/></svg>
    Insights
</a>
    <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_settings repo_branch_settings hooks integration_installations repo_keys_settings issue_template_editor /alstampe/Capstone---NLP-project/settings" href="/alstampe/Capstone---NLP-project/settings">
      <svg class="octicon octicon-gear" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 8.77v-1.6l-1.94-.64-.45-1.09.88-1.84-1.13-1.13-1.81.91-1.09-.45-.69-1.92h-1.6l-.63 1.94-1.11.45-1.84-.88-1.13 1.13.91 1.81-.45 1.09L0 7.23v1.59l1.94.64.45 1.09-.88 1.84 1.13 1.13 1.81-.91 1.09.45.69 1.92h1.59l.63-1.94 1.11-.45 1.84.88 1.13-1.13-.92-1.81.47-1.09L14 8.75v.02zM7 11c-1.66 0-3-1.34-3-3s1.34-3 3-3 3 1.34 3 3-1.34 3-3 3z"/></svg>
      Settings
</a>
</nav>


  </div>

<div class="container new-discussion-timeline experiment-repo-nav  ">
  <div class="repository-content ">

    

  
    <a class="d-none js-permalink-shortcut" data-hotkey="y" href="/alstampe/Capstone---NLP-project/blob/b41c6223224ad29cdea0f0ba27e9571472dee449/capstone%20report%20template.md">Permalink</a>

    <!-- blob contrib key: blob_contributors:v21:317f3dca8efe6e572f61633770b8a2d1 -->

    

    <div class="file-navigation">
      
<div class="select-menu branch-select-menu js-menu-container js-select-menu float-left">
  <button class=" btn btn-sm select-menu-button js-menu-target css-truncate" data-hotkey="w"
    
    type="button" aria-label="Switch branches or tags" aria-expanded="false" aria-haspopup="true">
      <i>Branch:</i>
      <span class="js-select-button css-truncate-target">master</span>
  </button>

  <div class="select-menu-modal-holder js-menu-content js-navigation-container" data-pjax>

    <div class="select-menu-modal">
      <div class="select-menu-header">
        <svg class="octicon octicon-x js-menu-close" role="img" aria-label="Close" viewBox="0 0 12 16" version="1.1" width="12" height="16"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
        <span class="select-menu-title">Switch branches/tags</span>
      </div>

      <div class="select-menu-filters">
        <div class="select-menu-text-filter">
          <input type="text" aria-label="Find or create a branch…" id="context-commitish-filter-field" class="form-control js-filterable-field js-navigation-enable" placeholder="Find or create a branch…">
        </div>
        <div class="select-menu-tabs" role="tablist">
          <ul>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="branches" data-filter-placeholder="Find or create a branch…" class="js-select-menu-tab" role="tab">Branches</a>
            </li>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="tags" data-filter-placeholder="Find a tag…" class="js-select-menu-tab" role="tab">Tags</a>
            </li>
          </ul>
        </div>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="branches" role="menu">

        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


            <a class="select-menu-item js-navigation-item js-navigation-open selected"
               href="/alstampe/Capstone---NLP-project/blob/master/capstone%20report%20template.md"
               data-name="master"
               data-skip-pjax="true"
               rel="nofollow">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"/></svg>
              <span class="select-menu-item-text css-truncate-target js-select-menu-filter-text">
                master
              </span>
            </a>
        </div>

          <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="select-menu-new-item-form js-new-item-form" action="/alstampe/Capstone---NLP-project/branches" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="3og1te7GDHqxxwV1HwNZ01em2x8Uq09gO/rk5xAt+y/nWWUJIaNPQSSkS0iteJXXJFpM/JQxoId0ucpljjF/2w==" />
            <input type="hidden" name="name" id="name" class="js-new-item-value">
            <input type="hidden" name="branch" id="branch" value="master">
            <input type="hidden" name="path_binary" id="path_binary" value="Y2Fwc3RvbmUgcmVwb3J0IHRlbXBsYXRlLm1k">

            <button type="submit" class="width-full select-menu-item js-navigation-open js-navigation-item">
              <svg class="octicon octicon-git-branch select-menu-item-icon" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 5c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v.3c-.02.52-.23.98-.63 1.38-.4.4-.86.61-1.38.63-.83.02-1.48.16-2 .45V4.72a1.993 1.993 0 0 0-1-3.72C.88 1 0 1.89 0 3a2 2 0 0 0 1 1.72v6.56c-.59.35-1 .99-1 1.72 0 1.11.89 2 2 2 1.11 0 2-.89 2-2 0-.53-.2-1-.53-1.36.09-.06.48-.41.59-.47.25-.11.56-.17.94-.17 1.05-.05 1.95-.45 2.75-1.25S8.95 7.77 9 6.73h-.02C9.59 6.37 10 5.73 10 5zM2 1.8c.66 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2C1.35 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2zm0 12.41c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm6-8c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
              <div class="select-menu-item-text">
                <span class="select-menu-item-heading">Create branch: <span class="js-new-item-name"></span></span>
                <span class="description">from ‘master’</span>
              </div>
            </button>
</form>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="tags">
        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


        </div>

        <div class="select-menu-no-results">Nothing to show</div>
      </div>

    </div>
  </div>
</div>

      <div class="BtnGroup float-right">
        <a href="/alstampe/Capstone---NLP-project/find/master"
              class="js-pjax-capture-input btn btn-sm BtnGroup-item"
              data-pjax
              data-hotkey="t">
          Find file
        </a>
        <clipboard-copy for="blob-path" class="btn btn-sm BtnGroup-item">
          Copy path
        </clipboard-copy>
      </div>
      <div id="blob-path" class="breadcrumb">
        <span class="repo-root js-repo-root"><span class="js-path-segment"><a data-pjax="true" href="/alstampe/Capstone---NLP-project"><span>Capstone---NLP-project</span></a></span></span><span class="separator">/</span><strong class="final-path">capstone report template.md</strong>
      </div>
    </div>


    
  <div class="commit-tease">
      <span class="float-right">
        <a class="commit-tease-sha" href="/alstampe/Capstone---NLP-project/commit/b41c6223224ad29cdea0f0ba27e9571472dee449" data-pjax>
          b41c622
        </a>
        <relative-time datetime="2018-12-02T10:30:39Z">Dec 2, 2018</relative-time>
      </span>
      <div>
        <a rel="author" data-skip-pjax="true" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42574791" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/alstampe"><img class="avatar" src="https://avatars0.githubusercontent.com/u/42574791?s=40&amp;v=4" width="20" height="20" alt="@alstampe" /></a>
        <a class="user-mention" rel="author" data-hovercard-type="user" data-hovercard-url="/hovercards?user_id=42574791" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/alstampe">alstampe</a>
          <a data-pjax="true" title="Completed version of report" class="message" href="/alstampe/Capstone---NLP-project/commit/b41c6223224ad29cdea0f0ba27e9571472dee449">Completed version of report</a>
      </div>

    <div class="commit-tease-contributors">
      
<details class="details-reset details-overlay details-overlay-dark lh-default text-gray-dark float-left mr-2" id="blob_contributors_box">
  <summary class="btn-link" aria-haspopup="dialog"  >
    
    <span><strong>1</strong> contributor</span>
  </summary>
  <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast " aria-label="Users who have contributed to this file">
    <div class="Box-header">
      <button class="Box-btn-octicon btn-octicon float-right" type="button" aria-label="Close dialog" data-close-dialog>
        <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
      </button>
      <h3 class="Box-title">Users who have contributed to this file</h3>
    </div>
    
        <ul class="list-style-none overflow-auto">
            <li class="Box-row">
              <a class="link-gray-dark no-underline" href="/alstampe">
                <img class="avatar mr-2" alt="" src="https://avatars0.githubusercontent.com/u/42574791?s=40&amp;v=4" width="20" height="20" />
                alstampe
</a>            </li>
        </ul>

  </details-dialog>
</details>
      
    </div>
  </div>



    <div class="file ">
      <div class="file-header">
  <div class="file-actions">


    <div class="BtnGroup">
      <a id="raw-url" class="btn btn-sm BtnGroup-item" href="/alstampe/Capstone---NLP-project/raw/master/capstone%20report%20template.md">Raw</a>
        <a class="btn btn-sm js-update-url-with-hash BtnGroup-item" data-hotkey="b" href="/alstampe/Capstone---NLP-project/blame/master/capstone%20report%20template.md">Blame</a>
      <a rel="nofollow" class="btn btn-sm BtnGroup-item" href="/alstampe/Capstone---NLP-project/commits/master/capstone%20report%20template.md">History</a>
    </div>

        <a class="btn-octicon tooltipped tooltipped-nw"
           href="https://desktop.github.com"
           aria-label="Open this file in GitHub Desktop"
           data-ga-click="Repository, open with desktop, type:windows">
            <svg class="octicon octicon-device-desktop" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M15 2H1c-.55 0-1 .45-1 1v9c0 .55.45 1 1 1h5.34c-.25.61-.86 1.39-2.34 2h8c-1.48-.61-2.09-1.39-2.34-2H15c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zm0 9H1V3h14v8z"/></svg>
        </a>

          <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="inline-form js-update-url-with-hash" action="/alstampe/Capstone---NLP-project/edit/master/capstone%20report%20template.md" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="aA4mX2NEnAcbZTtH4uC9jEeyBzelCJhyueyzvEhOaeDJFAMa/cbRt+POAmGD39/Lm6FfQpW/9coE0/2nljxXgg==" />
            <button class="btn-octicon tooltipped tooltipped-nw" type="submit"
              aria-label="Edit this file" data-hotkey="e" data-disable-with>
              <svg class="octicon octicon-pencil" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M0 12v3h3l8-8-3-3-8 8zm3 2H1v-2h1v1h1v1zm10.3-9.3L12 6 9 3l1.3-1.3a.996.996 0 0 1 1.41 0l1.59 1.59c.39.39.39 1.02 0 1.41z"/></svg>
            </button>
</form>
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="inline-form" action="/alstampe/Capstone---NLP-project/delete/master/capstone%20report%20template.md" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="9XzSeRVbUcaqnFJ2Tk3tKaOAyH9DJo+KhegH6YP9U/gO3cCk1SzUTy308G7a/SA2WfDiqICrs0/ODLhkAsveKA==" />
          <button class="btn-octicon btn-octicon-danger tooltipped tooltipped-nw" type="submit"
            aria-label="Delete this file" data-disable-with>
            <svg class="octicon octicon-trashcan" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 2H9c0-.55-.45-1-1-1H5c-.55 0-1 .45-1 1H2c-.55 0-1 .45-1 1v1c0 .55.45 1 1 1v9c0 .55.45 1 1 1h7c.55 0 1-.45 1-1V5c.55 0 1-.45 1-1V3c0-.55-.45-1-1-1zm-1 12H3V5h1v8h1V5h1v8h1V5h1v8h1V5h1v9zm1-10H2V3h9v1z"/></svg>
          </button>
</form>  </div>

  <div class="file-info">
      902 lines (570 sloc)
      <span class="file-info-divider"></span>
    56.4 KB
  </div>
</div>

      
  <div id="readme" class="readme blob instapaper_body">
    <article class="markdown-body entry-content" itemprop="text"><p>DNB Data Scientist for Enterprise Nanodegree Program</p>
<h2><a id="user-content-capstone-project" class="anchor" aria-hidden="true" href="#capstone-project"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Capstone Project</h2>
<p>Anne Line Stampe, DNB
November 30th, 2018</p>
<h2><a id="user-content-i-definition" class="anchor" aria-hidden="true" href="#i-definition"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>I. Definition</h2>
<h2><a id="user-content-project-overview" class="anchor" aria-hidden="true" href="#project-overview"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Project Overview</h2>
<p>Natural Language Processing (NLP) is a highly relevant topic in many business areas, and specifically so within DNB, Norways largest financial institution. The need for rapidly increasing levels of automation and digitalization spurs a demand for ways to reengineer processes based on manual handling of unstructured data. In addition to reduce costs and risks there is a clearly stated ambition to gain and use data-driven insight from these initiatives.</p>
<p>With a background from participation in an OCR-project piloting recently I chose a NLP-case as my Capstone project, uniting freshy aquired skills in the Nanodegree program and an interest in real-life document analysis.</p>
<p>NLP was briefly introduced in the Nanodegree course, giving a usefuil basis for this project.</p>
<p>My focus is to build skills and insights in possible realistic solutions for analysis of text in the form of full-scale documents rather than emails, tweets or other short, informal text objects. The data used in the project is text from 41 full books from the Project Gutenberg. For test purposes an additional unknown book and a separate text document is introduced at the end of the project.</p>
<p>As described in the project proposal my project covers three topics;</p>
<pre><code>Analysis of words in a full corpus; vocabulary, vector representation, word similarity
Methods for comparing individual text objects (books) for similarity based on vector representation
Methods for topic extraction from text objects, on a book level
</code></pre>
<p>The project has a high relevance for my job; in DNB we want to be able to understand documents by assessing vocabulary, comparing documents and extracting topics with algorithms as an alternative to a 100% manual reading and understandig the texts. There is also a need to be able to isolate specific data elements from documents (names, account numbers, values etc), but this is not a part of the Capstone scope. Because of this internal relevance, I plan to try out the project code on internal documents later.</p>
<h3><a id="user-content-problem-statement" class="anchor" aria-hidden="true" href="#problem-statement"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Problem Statement</h3>
<p>The underlying problem I want to address is the challenge we face when automating our processes in the bank; some tasks are by nature based on non-numerical data and the understanding of 'real language'. We still have to maintain sizeable staff for digesting large volumes of information manually with related cost and risk of human errors. There is a need to speed up the process and digitize results to be able to make data a part of a digital process. To harvest reusable insight from extracted information would be a valuable by-product. Fundamentally, the initiative can also add traction to the collaborative effort of transforming the bank to a more tech-driven institution and illustrate opportunities in the field of machine learning and AI.</p>
<p>Exploring NLP opportunities does not imply that all human handling of text is expandable, there are numerous situations where we must have skilled people reading and analyzing text based on more non-procedural rules, but it is a start on a journey to a more digital bank.</p>
<p>I go into this project with a moderate ambition of precision from the document analysis, but expect to be able to confirm the hypothesis stated in the three questions. For such a complicated field, a modest proof of concept is a good basis for believing in results from a possible full-scale project later.</p>
<h3><a id="user-content-motivation-for-the-three-chosen-problem-statements" class="anchor" aria-hidden="true" href="#motivation-for-the-three-chosen-problem-statements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Motivation for the three chosen problem statements</h3>
<p>The three problems are formed both for building skills and proving NLP capabilities and for practical purposes.</p>
<p>The reason for these three problem statements and how they relate to my practical challenge;</p>
<p>Word analysis and similarity checks will support an automated analysis of large documents by giving a 'heads-up' of the                 vocabulary and text content, ie by listing frequent words in a document (as an alternative to Topic Extraction).
In Norway most people are quite fluent in English, but being able to 'on the fly' look for similar words in the text / document can     be a guidance for understanding the document and increase language sensibility.</p>
<p>Document comparisons can be useful when looking at large documents, I can imagine doing this when starting a business days'             batch of received Trade Finance or Loan documents; being able to compare new documents with a library of historical documents to       identify the most similar objects. In a team, people specialice on fields and we often want to direct cases to the one with the         most relevant skillset on a topic or perhaps on a client or a customer segment. As these document have a standard front page, the       case content is not always identifyable by first glance only.
An alternative use-case can be to retrieve older documents similar to a newly arrived, to investigate how a similar historical case was handled. Another idea is to use this for finding texts written by the same person if this information is not available elsewhere.</p>
<p>Finally, the value of topic extraction is interesting for several situations, some similar to the above mentioned. Backoffice processes in DNB still handles large numbers of extensive documents, and a topic extract will be helpful for sorting and prioritizing. If numbers of words in extract are increased it could help preparing for meetings based on insight in large reports.</p>
<h3><a id="user-content-strategy-for-the-work" class="anchor" aria-hidden="true" href="#strategy-for-the-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Strategy for the work</h3>
<p>All text analysis, as other ML tasks, starts with understanding the data, the questions asked and the tools and methods needed to answer the questions with available data.</p>
<h4><a id="user-content-building-more-relevant-skills" class="anchor" aria-hidden="true" href="#building-more-relevant-skills"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Building more relevant skills</h4>
<p>Before starting to code, I needed some more nlp-specific knowledge. I have used the Kindle book below which I spent some time reading before getting at the coding part.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49142891-17369900-f2fa-11e8-9f87-83cfd2cf201f.png"><img src="https://user-images.githubusercontent.com/42574791/49142891-17369900-f2fa-11e8-9f87-83cfd2cf201f.png" alt="screenshot 7" style="max-width:100%;"></a></p>
<h4><a id="user-content-gathering-and-loading-input-data" class="anchor" aria-hidden="true" href="#gathering-and-loading-input-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gathering and loading input data.</h4>
<p>The data for this project is solely books from Project Gutenberg, with the exeption of a single document to be testet at the very end of the project. All books will be downloaded on my laptop and then uploaded to my Notebook, to be read into dataobjects.</p>
<h4><a id="user-content-preprocessing" class="anchor" aria-hidden="true" href="#preprocessing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Preprocessing</h4>
<p>Preprocessing text data is partly similar to cleaning numerical and categorical data, partly very different. The options and choices are many, hence this step is important and represents a large part of the projects time and effort. Preprocessing code for all three parts of the project scope will be prepared and tested before the data transformation starts.</p>
<h4><a id="user-content-transformation-training-and-producing-results" class="anchor" aria-hidden="true" href="#transformation-training-and-producing-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transformation, training and producing results</h4>
<p>Transforming the text data by establishing models, vectorization, dimensionality reduction, training the model and producing results will be more straightforward and similar to numerical-oriented projects, but steps and methods must be appropriate to the case and results visualized to inspect language-oriented elements.</p>
<h4><a id="user-content-inspect-and-assess" class="anchor" aria-hidden="true" href="#inspect-and-assess"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Inspect and assess</h4>
<p>Results from transformation of text can be visualized in addition to list-based output.</p>
<h4><a id="user-content-test-of-an-unknown-additional-book-from-gutenberg-and-a-contemporary-text-document" class="anchor" aria-hidden="true" href="#test-of-an-unknown-additional-book-from-gutenberg-and-a-contemporary-text-document"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test of an 'unknown' additional book from Gutenberg and a contemporary text document</h4>
<p>Finally I will do a test on an ebook and a text document which are both unknown and new. The test will be of the Topic extraction.</p>
<h4><a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation</h4>
<p>Results will be discussed.</p>
<h4><a id="user-content-deliverables" class="anchor" aria-hidden="true" href="#deliverables"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Deliverables</h4>
<p>Project deliverables will primarily be in form of results described in the report with detailed descripiton of code, models and figures. All results are commented and discussed.</p>
<p>In addition all code, test and comments are present in the project Notebook. All books reside on GitHub.</p>
<p>The Notebook is quite extensive as it includes all code used for testing different ways to preprocess the text, plus some vectorization attempts which I did not utilize: Hence, no not every part of the Notebook is used for the final solution but has been important to learn more about the many ways to handle text data.</p>
<h3><a id="user-content-metrics" class="anchor" aria-hidden="true" href="#metrics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Metrics</h3>
<p>As described in the capstone project proposal, the three parts of my project are all of a nature where sharp metrics are not always present. The individual tasks will produce some comparable values, but more important is the subjective opinion ie:</p>
<h4><a id="user-content-word-similarity" class="anchor" aria-hidden="true" href="#word-similarity"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Word similarity</h4>
<pre><code>Does the vocabulary analysis give me a useful insight n the documents' nature?
Are the similarity suggestions on single words correct according to my language understanding'?
Does the word pairing examples on the vectorized data make sense?'

Main question: 'Would I trust the outcome of the nlp enough to replace a human reading the whole document?' 
</code></pre>
<h4><a id="user-content-book-similarity" class="anchor" aria-hidden="true" href="#book-similarity"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Book similarity</h4>
<pre><code>'Does the book similarity look sensible, given my knowledge of the books?'.   
'Do some of the similarity results give me a new and useful insights in the document contents?'

Main question: 'Do I trust the results to be a useful tool in my workday?'
'Can I maybe even believe in gaining more insight and efficiency by seeing patterns in larger volumes of documents?'  
</code></pre>
<h4><a id="user-content-topic-extraction" class="anchor" aria-hidden="true" href="#topic-extraction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Topic extraction</h4>
<pre><code>'Is this a readable and good topic extraction from the given book?'

Main question: 'Do I ses this as a reliable tool for my collegues to use for fast intro to text content?'   
</code></pre>
<h2><a id="user-content-ii-analysis" class="anchor" aria-hidden="true" href="#ii-analysis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>II. Analysis</h2>
<h3><a id="user-content-data-exploration-and-description" class="anchor" aria-hidden="true" href="#data-exploration-and-description"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Exploration and description</h3>
<p>Data for the project is in the form of a 'mini-library' of 41 books collected from the project Gutenberg.
Project Gutenberg is a collection of freely available ebooks on various formats, forming a huge historical library of literature.</p>
<p>'Project Gutenberg offers over 57,000 free eBooks. Choose among free epub books, free kindle books, download them or read them online. You will find the world's great literature here, with focus on older works for which copyright has expired. Thousands of volunteers digitized and diligently proofread the eBooks, for enjoyment and education'. (From the website <a href="http://www.gutenberg.org" rel="nofollow">www.gutenberg.org</a>)</p>
<p>As a balance between informative corpus size and variety - and a volume manageable for computation on a standard laptop - I decided to use 41 books for my mimi-library. The choice of books is based on :</p>
<pre><code>The books are all in english
6 books have Norwegian or Danish origin (Ibsens plays and 2 fairytale collections)
Some of the items are plays (Ibsen, Shakespeare)
I have personally read all books - primarily wholly, but for some only partially. 
Some authors are represented by several works (Verne, Ibsen, Dumas, Burroughs)
Nearly half of the items can be described as 'adventure journey tales'.
Two of the authors (Shakespeare and Ibsen) are known for their very rich vocabulary(29.000 and 27.000).  
</code></pre>
<p>My list, as named in the file folder:</p>
<pre><code>'A_Dolls_house_Ibsen.rtf',
'Alice_in_Wonderland.rtf',
'All_around_the_moon_Verne.rtf',
'An_archtartic_mystery_verne.rtf',
'Anthem_Rand.rtf',
'Around_the_world_in_80_days_Verne.rtf',
'Don_Quixote.rtf',
'Fairytales_H_C_Andersen.rtf',
'Ghosts_Ibsen.rtf',
'Great_Expectations_by_Charles_Dickens.rtf',
'Gullivers_Travels_Swift.rtf',
'Hedda_Gabler_Ibsen .rtf',
'Iliad_Homer.rtf',
'Jungle_tales_of_Tarzan.rtf',
'LIttle_Eyolf_Ibsen.rtf',
'Martin_Eden_Jack_London.rtf',
'Meditations_Aurelius.rtf',
'Metamorphosis _ Kafka.rtf',
'Norwegian_tales_Asbjornsen_Moe.rtf',
'Peter_Pan.rtf',
'Pride_and_Prejudices .rtf',
'Robinson_Crusoe _ Defoe.rtf',
'Shakespeare.rtf',
'Siddharta _ Hesse.rtf',
'Swanns_Way_Proust.rtf',
'Tale_of_two_cities_dickens.rtf',
'Tarzan_of_the_apes.rtf',
'Tarzan_the_terrible.rtf',
'The_3_musketeers_Dumas.rtf',
'The_Iron_Heel_Jack_london.rtf',
'The_beasts_of_Tarzan.rtf',
'The_black_tulip_Dumas.rtf',
'The_brothers_karamazov_Dostoyevsky.rtf',
'The_importance_of_being_earnest_Wilde.rtf',
'The_jungle_book_Kipling.rtf',
'The_man_in_the_iron_mask_Dumas.rtf',
'The_return_of_tarzan.rtf',
'The_secret_of_the_island_verne.rtf',
'The_trial_Kafka.rtf',
'War_and_peace_tolstoy.rtf',
'Wuthering_Heights_bronte.rtf'
</code></pre>
<p>Although the filenames indicate .rtf (Rich Text Format), they are in straightforward text format. I started with a few rft format files, bud decided to download the .txt format which I realized is available for all my books. The file naming was kept for the sake of convenience as the code reading the files then was written for the .rtf extension. All books are present in a folder in GitHub.</p>
<h4><a id="user-content-comments-on-the-book-data" class="anchor" aria-hidden="true" href="#comments-on-the-book-data"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comments on the book data</h4>
<p>After a few assessments of the data on a word level I noticed some parts of the vocabulary that I did not expect from older works, such as 'email'. I had screened through the first part of most of the books to verify the content and format, which all seemed correct. At closer inspection I realized that all ebooks had a standard, quite long, section at the end, containing legal description of the ebook handling. In addition to the introduction of unwanted irrelevant vocabulary to the corpus this part is the same in all books and would therefore reduce the uniqueness of the items. The section was for this reason removed from each of the books.</p>
<p>The complete volume of words from the books, the 'corpus' was analyzed thouroughly.
There are many levels (and combination of levels) to look at the corpus;</p>
<pre><code>Corpus level
Book level
Sentence level
Vocabulary level (unique words)
Word level (real words, or 'tokens')
Level of 'words' before removing non-meaningful characters.  
Level of 'reduced tokens'; ie words cleaned and/or transformed to a netted minimum.   
</code></pre>
<p>Below is an example of numbers for these levels</p>
<pre><code>Corpus_raw: 30776216
Wordlist: 5572533
Sentences: 309218
Vocabulary: 96909
</code></pre>
<p>As a reflection 96909 comes across as a very rich vocabulary, even with Shakespeare and Ibsen onboard. One reason is probably that names are included in the corpus and are counted as words. Linguists state that if we include all varieties of known english words it will add up to ca 500.000. A normal active vocabulary consists of a modest 5-6000 words in most countries.</p>
<p>One of the first transforms on the corpus was done using the gensim.utils.simple_preprocess, which performs this :
'Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long'.
<a href="https://programtalk.com/python-examples/gensim.utils.simple_preprocess/" rel="nofollow">https://programtalk.com/python-examples/gensim.utils.simple_preprocess/</a>.</p>
<p>On inspection this returns a compact list of tokens, the number of words reduced significantly, ie for Alice in wonderland, from 53690 raw words to 9401 tokens. Netting down to unique tokens, in effect the 'token vocabulary' shows a modest token vocabulary of 1504.</p>
<p>Although utilities with options such as the 'simple_preprocess' are useful, I wanted to test effects of the preprocessing in detail, and spent time trying out different functions on sentence and word levels, coding small functions with basic tokenizers and cleaning.</p>
<h4><a id="user-content-assessing-different-word_tokenizers" class="anchor" aria-hidden="true" href="#assessing-different-word_tokenizers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Assessing different word_tokenizers</h4>
<p>Tokenizers in common utility libraries offers a variety of processing options for establishing 'tokens'; separate word objects. As an illustrative example, I tested the different outcomes of two often used word tokenizers applied on sentences created from the punct_sentence_tokenizer;</p>
<pre><code>nltk.tokenize.word_tokenize(sentence) versus nltk.tokenize.wordpunct_tokenize(sentence)
</code></pre>
<p>For sentence no:99 in the Alice in Wonderland corpus we can see how the word tokenizers differ, respectively:</p>
<pre><code>'she was up to her chin in salt-water.',
['she', 'was', 'up', 'to', 'her', 'chin', 'in', 'salt-water', '.'],
['she', 'was', 'up', 'to', 'her', 'chin', 'in', 'salt', '-', 'water', '.']
</code></pre>
<p>The word_tokenizer keeps the hyphenated words as one token, the wordpunct_tokenize splits these words into 3 separate tokens.  When analyzing text in literary works, it makes sense to keep the original word as written by the author, this sentence is a good example as the words 'salt' and 'water' separately is quite different from 'salt-water'.  The first could be a part of a recipe for baking bread, the second indicates seawater - or tears, as is the case in this story.</p>
<p>I choose to use the word_tokenizer for this book-based project, although another context could be a case for preferring the  wordpunct_tokenizer. Use of hyphenation varies between languages, is said to be declining in the English language and is less common in Norwegian. A discussion on tokenizers is presented here:
<a href="https://www.researchgate.net/publication/264157595_A_Comparison_of_13_Tokenizers_on_MEDLINE" rel="nofollow">https://www.researchgate.net/publication/264157595_A_Comparison_of_13_Tokenizers_on_MEDLINE</a></p>
<h4><a id="user-content-cleaning-the-sentences-for-stopwords" class="anchor" aria-hidden="true" href="#cleaning-the-sentences-for-stopwords"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cleaning the sentences for stopwords</h4>
<p>When preparing for topic analysis and other content assessments we want to look at the meaningful elements of the text, not the 'filler words'. There are many lists of 'filler words' or 'stop words' as they are called, for several languages. I use a list of english 'stop words' (from the nltk.corpus) and write two small code snippets to remove these words in a sentence and then loop for all sentences in a book. The function 'make-fin-sent' (below) will 'make final sentences' for a given book, to be used for topic extraction where we want to avoid stopwords:</p>
<pre><code>def remove_stopwords(words):
   return [word for word in words if word not in stop_words]

def make_fin_sent(sent_in):
    fin_sent=[]
    for sent in sent_in:
        fin_sent.append(remove_stopwords(sent))
    return fin_sent
</code></pre>
<p>A final sentence will typically look like this (sentence no 80 in Swanns Way):</p>
<pre><code>['Come', 'stop', 'husband', 'drinking', 'brandy']
</code></pre>
<h4><a id="user-content-looking-at-frequent-words-in-a-text" class="anchor" aria-hidden="true" href="#looking-at-frequent-words-in-a-text"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Looking at frequent words in a text</h4>
<p>Zooming on on the corpus, the need for data cleaning soon becomes obvious. First try of 'most common words' on a raw (uncleaned) corpus returns a rather uninteresting list over the 10 most frequent 'words', using FreqDist (from nltk.probability import FreqDist).</p>
<pre><code>({',': 460770, 'the': 270580, '.': 258286, 'and': 160660, 'of': 143300, 'to': 142159, 'a': 95923, 'I': 91671, 'in': 78754, 'that': 71929, ...})
</code></pre>
<p>Removing punctiations and single characters with a small cleaning function improves a little:</p>
<pre><code>cleanlist=[]
def sentence_to_cleanlist(raw):
    cleanlist = re.sub("[^a-zA-Z]"," ", raw)
    return cleanlist

({'the': 270904, 'and': 161240, 'to': 143756, 'of': 143437, 'a': 96535, 'I': 92847, 'in': 79205, 'that': 72439, 'he': 64047, 'his': 58524, ...})
</code></pre>
<p>This exercize can be done for the individual books, but across english books the top 20 words pretty much remains the same.</p>
<p>Next step is to remove the stop words, as described above. We can see that all 10 words on the previous list were in fact stopwords.</p>
<p>Using the result from removing stopwords with make_fin_sent, the 10 most frequent words for the full corpus are:</p>
<pre><code>('said', 19290),
('one', 17840),
('would', 17020),
('man', 11407),
('could', 11068),
('upon', 9815),
('well', 9252),
('time', 9081),
('know', 8822),
('thou', 8765)
</code></pre>
<p>Looking at this list, I can recognize the words of literature objects and dialogue-oriented text, with 'said' on top of the lists.<br>
The last word is a hint of the book's age and I guess this is a Shakespearian word.</p>
<p>Below is a figure showing the count of the 30 most common words in the corpus, after prereprocessing as described.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49156595-06981a00-f31e-11e8-8e8f-f7007265d8b9.png"><img src="https://user-images.githubusercontent.com/42574791/49156595-06981a00-f31e-11e8-8e8f-f7007265d8b9.png" alt="image" style="max-width:100%;"></a></p>
<p>Applying the word frequency on only Alice in Wonderland, without stopwords, we get this list:</p>
<pre><code> ('alice', 173),
 ('said', 144),
 ('little', 59),
 ('rabbit', 37),
 ('one', 35),
 ('like', 34),
 ('queen', 30),
 ('could', 28),
 ('mouse', 27),
 ('illustration', 26)
</code></pre>
<p>For 'The junglebook' , the 10  most frequent words when cleaned for filler words are (using FreqDist again):</p>
<pre><code>'said': 430, 'little': 231, 'mowgli': 220, 'man': 177, 'one': 174, 'would': 162, 'jungle': 147, 'head': 137, 'bagheera': 129, 'could': 125, 
</code></pre>
<h4><a id="user-content-recap---data-cleaning-in-nlp" class="anchor" aria-hidden="true" href="#recap---data-cleaning-in-nlp"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Recap - data cleaning in nlp</h4>
<p>Cleaning the corpus can be done by several small cleaning code-snippets;</p>
<pre><code>sentence tokenization
word tokenizations
removing unwanted characters (often called normalization)
converting all words to lowercase  
removing stopwords
lemmming and stemming
pos-tagging 
</code></pre>
<p>The 5 first cleaning steps are described. The two latter were tested on the corpus, but not used for later transformation purposes.
Code for these tests are still kept in the Notebook.</p>
<h3><a id="user-content-exploratory-visualization" class="anchor" aria-hidden="true" href="#exploratory-visualization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Exploratory Visualization</h3>
<p>Word frequency after different word preprocesses is interesting. Below are two more plots describing this, showing number of words in the whole corpus and in one book. Both are cleaned by normalization (removing stray characters), the book is cleaned for stopwords.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49230127-3f53f400-f3ef-11e8-932d-60167007b8a9.png"><img src="https://user-images.githubusercontent.com/42574791/49230127-3f53f400-f3ef-11e8-932d-60167007b8a9.png" alt="image" style="max-width:100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49230156-51ce2d80-f3ef-11e8-827b-c777b952b2e2.png"><img src="https://user-images.githubusercontent.com/42574791/49230156-51ce2d80-f3ef-11e8-827b-c777b952b2e2.png" alt="image" style="max-width:100%;"></a></p>
<p>More visuals are provided later on the word analysis task.</p>
<h4><a id="user-content-algorithms-and-techniques" class="anchor" aria-hidden="true" href="#algorithms-and-techniques"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Algorithms and Techniques</h4>
<p>Algorithms used for the projects are chosen based on my understanding of useful techniques, based on learning in the Nanodegree, the book I have read and supplementing articles.</p>
<p>As described in the book, there are three commonly used ways to treat text analysis, often by combinations of the three;</p>
<pre><code>  The Gensim way
  The nltk way
  The ScikitLearn way
</code></pre>
<p>My use of algorithms are mainly by using Gensim utilities, but also nltk resources.</p>
<p>The data preprocessing was described in previous part of the report;</p>
<pre><code>  Use of word- and sentence tokenizers from nltk.tokenize
  Use of nltk.probability.FreqDist
  Use of nltk.corpus.stopwords 
  Use of gensim.utils.simple_preprocess
</code></pre>
<p>Word and document analysis</p>
<pre><code>  gensims Word2Vec and Doc2Vec for vectorization of corpus
  gensim similarities for similarity analysis
  sklearn.manifold.TSNE for dimension reduction
</code></pre>
<p>Topic extraction</p>
<pre><code>  gensim Dictionary and LDAmodel from gensim.models
</code></pre>
<p>For all algorithms the initial parameters are set based on examples from the Udacity courses, the book, or articles. Some are set from characteristics of the date (ie vocabulary size). Many of the parameters are changed through the project to understand how and if the models can be optimized.</p>
<h3><a id="user-content-algorithms-used---how-do-they-work" class="anchor" aria-hidden="true" href="#algorithms-used---how-do-they-work"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Algorithms used - how do they work?</h3>
<p>Although we often refer to Word2Vec as one algorithm, it is actually a group of similar models, as described in Wikipedia :</p>
<pre><code>'Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks     that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a         vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in     the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in      close proximity to one another in the space.'
</code></pre>
<p>This definintion describes well the main outline of this project; the algorithm taking a large text corpus and procucing a vector space where similar words are placed close to each other'. . In the project the vector space is dimensionally reduced to be able to look at the result in a x/y axis.The word 'word embedding' is used to the describe the process of creating numerical vectors from text.</p>
<p>I like the exapmple most used for this vector representation, the king- queen vector representation</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49329981-1f0b6d00-f588-11e8-8f72-eca2aa01284b.png"><img src="https://user-images.githubusercontent.com/42574791/49329981-1f0b6d00-f588-11e8-8f72-eca2aa01284b.png" alt="image" style="max-width:100%;"></a></p>
<p>It is important to understand that these algorithms are specifically designed to be applied on text data. If we dig deeper into the use of the models there is normally a choice of technique between bag-of-words and skip-grams. The difference can be seen as 'from word to context' vs 'from context to word', or, more precicely described from Wikipedia again :</p>
<pre><code>'In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. 
In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.'
</code></pre>
<p>Both has its pros and cons as to accuracy and performance. In the project, bag-of-words is used. The parameter 'context window' is related to this,saying how may words are regarded as the 'context' for the algorithm.</p>
<p>The algorithms are trained on the project corpus, as described. The larger the corpus, the richer the vocabulary. Of course, this approach will limit the word knowledge to the words we feed the algorithm. If I try to find or compare a to word not present in my books, it will fail. This is very different from a number-based model which 'knows' all possible numbers not in the training set.</p>
<h3><a id="user-content-tsne-for-enabling-visuals" class="anchor" aria-hidden="true" href="#tsne-for-enabling-visuals"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TSNE for enabling visuals</h3>
<p>'t-SNE stands for t-Distributed Stochastic Neighbor Embedding. It visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. t-SNE is good at creating a map that reveals structure and embedding relationships at many different scales. This is particularly important for high-dimensional inter-related data that lie on several different low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints.'</p>
<p>As described in the article 't-SNE separates and clusters group of words that we know semantically similar or different respectively', and is therefore often used for text analysis.</p>
<p>ref:<a href="https://www.codeproject.com/tips/788739/%2fTips%2f788739%2fVisualization-of-High-Dimensional-Data-using-t-SNE" rel="nofollow">https://www.codeproject.com/tips/788739/%2fTips%2f788739%2fVisualization-of-High-Dimensional-Data-using-t-SNE</a></p>
<h3><a id="user-content-lda-for-topic-extraction" class="anchor" aria-hidden="true" href="#lda-for-topic-extraction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Lda for topic extraction</h3>
<p>Lasty, I use gensims Lda algorithm for topic extraction, described by Wikipedia below and more detailed in the article linked here</p>
<pre><code>In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations     to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words       collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is       attributable to one of the document's topics. LDA is an example of a topic model.
</code></pre>
<p><a href="https://blog.imaginea.com/lda-nlp-and-code-analysis/" rel="nofollow">https://blog.imaginea.com/lda-nlp-and-code-analysis/</a></p>
<p>Using plate notation the Bayesian network for LDA would look like the figure below giving an illustration of the number of documents (M), words(N) and topics (K). Alpha, betha, theta and rho represents parameters and topic distribution.<br>
The article describes the maths for the lda, with a short summary:</p>
<pre><code>Tokenize the documents into collection of words.
Find sensible values for aplha, beta and K.
Initialize random topic for each word in the corpus.
Repeat a Gibbs iteration (sample-based derivation and update) until sufficient result
For each word of each document, calculate values for each topic k – 
Assign as the topic k (from among total K topics) according to the calculated proability
</code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49337193-c97aa300-f60f-11e8-8e38-01dd2048084c.png"><img src="https://user-images.githubusercontent.com/42574791/49337193-c97aa300-f60f-11e8-8e38-01dd2048084c.png" alt="image" style="max-width:100%;"></a></p>
<p>Teh Lda model algorithm was used on a matrix, using bag-of-words with gensims doc2bow and a corpora dictionary.</p>
<h3><a id="user-content-benchmark" class="anchor" aria-hidden="true" href="#benchmark"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmark</h3>
<p>A benchmark for performance for this project is somewhat challenging as the 'real-life' benchmark starts with time and cost for a person reading and understanding text. Such time-and cost values not often used as benchmark in these experiments. Nevertheless, these figures will be the basis for a business case if the solution is to be evaluated for a project, measured against the project costs.</p>
<p>With an average adult reading speed of 250 words per minute a book with 90.000 words (estimated average lenght of a novel) will be read in 360 minutes; 6 hours. For my bank case, a relevant comparison would be a document of perhaps 5000 words; estimated reading time being 20 minutes. The act of analysis and documenting the results will represent additional time and effort. Obviously, a machine-based read, classification and topic extract will represent a major performance increase.</p>
<p>If we rather see the benchmark as the 'first state of the solution' , to be compared to the final version the report describes and discusses the model improvements. The most prominent improvements were not by fine-tuning the complex models, but from the level of cleaning in the preprocess stage. A raw corpus can be seen as a very unbalanced dataset, where large volumes of single characters and stopwords skew the corpus and make topic extraction nearly impossible.</p>
<h2><a id="user-content-iii-methodology" class="anchor" aria-hidden="true" href="#iii-methodology"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>III. Methodology</h2>
<h3><a id="user-content-data-preprocessing" class="anchor" aria-hidden="true" href="#data-preprocessing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Preprocessing</h3>
<p>NLP requires several steps of preprocessing just as we do for numerical values and categorical data. The methods are different, but there are several rich libraries of code available. In effect, the first challenge is to understand which packages to apply. Data cleaning for nlp differs from cleaning of datasets for numerical analysis - the choice of preparing must be made based on the purpose of the analysis, we can seldom do an all-purpose text cleaning.</p>
<p>I realized that the three separate questions posed in the project would have to be answered with three separate paths with different data prepping and processing.</p>
<ol>
<li>Data analysis of all words in all books in the mini-library
With a focus on the sum of all words in the book collection the preprocessing starts with a 'full' collection of words across the books. The text is then divided into sentences and all words are identified separately. The books are structured into sentences, using a trained process. I also use a word-oriented vectorization and eventually look at the matrix of all words, identifying the most frequent words and how the words are related to each other in the matrix.</li>
</ol>
<p>In the analysis process I keep separate versions of the result of different text preparations, to be in control of the results and which versions are suitable for the next step of analysis.</p>
<ol start="2">
<li>
<p>Data analysis for comparing books in the mini-library. ,<br>
When aiming to compare text objects I need to separate the objects(books) and prepare them individually, building structures for mathematical comparisons. For this purpose I use another version of the package, with a document(ie book) focus. Each book is vectorized and the vectors are compared.</p>
</li>
<li>
<p>Data prepping and analysis for topic extraction
Finally, the prepping for topic extraction will reuse some of the prep steps, but with a twist; the text is divided into books again, this time on a corpus level. All analysis is done on book level. In addition to constructing sentences and cleaning for unneccesary characters, the text is made into lower characters and all stopwords are removed explictly. Stopwords in the text makes it difficult to extract topics and forms unneccessary 'noise'. This was very clearly visible from the code run I performed before and after stopword removal.</p>
</li>
</ol>
<h3><a id="user-content-implementation" class="anchor" aria-hidden="true" href="#implementation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation</h3>
<p>Transformation of data after preprocessing</p>
<p>Stepping up the analysis after preprocessing is done by vectorizing the corpus.There are numerous different ways to do this and quite tricky to navigate among the options.I chose to use the Gensim functionality for vectorizing. Gensim offers vetorization option on document and word level, with word2Doc and Word2Vec.</p>
<h3><a id="user-content-1-word-analysis-with-word2vec" class="anchor" aria-hidden="true" href="#1-word-analysis-with-word2vec"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1) Word analysis with Word2Vec</h3>
<p>Gensim has a reknown Vectorization option, the Word2Vec which, as the name implies, vectorizes the corpus on a word level. Each unique word is assigned a numerical value, giving av matrix where one dimension is the size of the vocabulary.</p>
<p>I start by establishing a word-based Word2Vec model; named 'word_model'.</p>
<p>The word_model is applied on the whole corpus tokenized on a sentence-level, using the version called 'sentences'. In 'sentences', the full corpus is split into sentences and each word is tokenized.</p>
<p>Sentence no 5678 in the corpus is a good example:</p>
<pre><code>'when','we','get','to','the','moon','what','shall','we','do','there'
</code></pre>
<p>The word_model is set by :</p>
<pre><code>Parameters:
num_features = 300
min_word_count = 15
num_workers = multiprocessing.cpu_count()
context_size = 10
downsampling = 1e-4
seed = 9

word_model = w2V.Word2Vec(parameters)
word_model.build_vocab(sentences)
print("model vocabulary length:", len(word_model.wv.vocab))
model vocabulary length: 14661
</code></pre>
<p>Confirming the model after building vocabulary:</p>
<pre><code>Word2Vec(vocab=14661, size=300, alpha=0.025)
word_model.train(sentences, epochs=5,
total_examples=14661)

all_word_vectors_matrix = word_model.wv.vectors
</code></pre>
<p>The resulting matrix is huge (14661, 300). To be able to visualize and look into the matrix I use TSNE to reduce the dimensions to 2.</p>
<pre><code>tsne = sklearn.manifold.TSNE(n_components = 2, 
                         early_exaggeration = 6,
                         learning_rate = 500,
                         n_iter = 300,
                         random_state = 2)

 all_word_vectors_matrix_2dim = tsne.fit_transform(all_word_vectors_matrix)
</code></pre>
<p>The matrix values are set into a dataframe for inspection and visuals. The shape is the vocabulary size (14661, 2).
On this dataframe I can 'look at' the word data in a frame with visuals.
All the 14661 words are placed along the x- and y-axis, forming a circle-like cloud.
I can 'zoom in' on elements in the frame, by navigating along the x- and y-axis.</p>
<p>When picking a word the coordinates are given , whowing where the word resides. If I 'zoom in' on the area around the chosen word by picking a slice of the frame by the coordinates, I can see which words are in the close vicinity vector-vise.
A few examples:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49079447-de86b900-f240-11e8-929c-519de82fb7ac.png"><img src="https://user-images.githubusercontent.com/42574791/49079447-de86b900-f240-11e8-929c-519de82fb7ac.png" alt="image" style="max-width:100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49079567-31607080-f241-11e8-86e4-cefdea35dffb.png"><img src="https://user-images.githubusercontent.com/42574791/49079567-31607080-f241-11e8-86e4-cefdea35dffb.png" alt="image" style="max-width:100%;"></a></p>
<p>Most similar words - in the vector dimension</p>
<p>Below are two examples, the first on the initial model version, the second on an enhanced model version when upping some of the parameters (number of features, context size) . The results were not dramatically improved for this task after model tweaking.</p>
<pre><code>word_model.wv.most_similar('pride')

('envy', 0.7191991806030273),
 ('ambition', 0.7174361348152161),
 ('meekness', 0.7124776244163513),
 ('fortitude', 0.7029587030410767),
 ('chastisement', 0.7014052271842957),
 ('disdain', 0.7011118531227112),
 ('hypocrisy', 0.7005854845046997),
 ('penitence', 0.6966824531555176),
 ('cowardice', 0.696437656879425),
 ('deceitful', 0.6881036758422852)

word_model.wv.most_similar('hunger')

('thirst', 0.8457642197608948),
 ('pangs', 0.7334113121032715),
 ('fatigue', 0.7226671576499939),
 ('gnawing', 0.7220444679260254),
 ('starved', 0.6977795362472534),
 ('exhaustion', 0.6882658004760742),
 ('famine', 0.6871694922447205),
 ('hungry', 0.6860570907592773),
 ('sickness', 0.6858627796173096),
 ('craving', 0.6856997013092041)
</code></pre>
<h4><a id="user-content-word-pairing" class="anchor" aria-hidden="true" href="#word-pairing"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Word pairing</h4>
<p>Finally, I want to try out the classic 'king-to-queen' vector comparison.
For this purpose I use the most_similar_cosmul functionality, based on a cosine computation.</p>
<p>The base for this is a ' x relates to y as a relates to...' logic, expecting to return the word that has a menaingful relation to a as y has to x. My attempts to do this were not in the first tryouts as clear as the reknown queen-king case. I thought the reason was a limited corpus, but changing the model parameters improved the result, as shown below.</p>
<pre><code>def nearest_similarity_cosmul(start1, end1, start2):
    similarities = word_model.wv.most_similar_cosmul(
    positive=[start1, start2],
    negative=[end1])
end2 = similarities[0][0]
print("{start1} is related to {end1}, as {start2} is related to {end2}".format(**locals()))
return end2
</code></pre>
<p>First version of model returns:</p>
<pre><code>'ocean is related to submarine, as jungle is related to forest
swann is related to odette, as nora is related to helmer (OK!)
queen is related to king, as woman is related to girl'
</code></pre>
<p>After changing parameters the similarity tests are better;</p>
<pre><code>nearest_similarity_cosmul("man", "woman", "queen")
nearest_similarity_cosmul("husband", "wife", "man")
nearest_similarity_cosmul("swann", "odette", "artagnan")
nearest_similarity_cosmul("sea", "boat", "city")

'man is related to woman, as queen is related to king
husband is related to wife, as man is related to woman
swann is related to odette, as artagnan is related to porthos
sea is related to boat, as city is related to palaces'
</code></pre>
<h3><a id="user-content-2-book-comparisons-using-doc2vec" class="anchor" aria-hidden="true" href="#2-book-comparisons-using-doc2vec"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2) Book comparisons, using Doc2Vec</h3>
<p>Dov2Vec is similar to Word2Vec, but applied on document (here book) level.</p>
<p>A hypothesis was to use tfidf here, but reading articles seems to indicate that LDA does not need TF-IDF and can be used with bag_of_words only. There are different opnions on whether TF-idf will improve the results, but without clear conclusions. This is also commented later in the report.</p>
<p>The model is defined and named model_doc. Data input is 'book_corpus', the book-level corpus established earlier.<br>
The model uses 3 parameters. All were varied during the project, the results seemed to stay at fairly the same level.</p>
<pre><code>model_doc = gensim.models.Doc2Vec(vector_size = 300, 
                          min_count = 3, 
                          epochs = 10)
                          
model_doc.build_vocab(book_corpus) 
model's vocabulary length: 31862

model_doc.train(book_corpus, total_examples=31862, epochs=10)
</code></pre>
<p>Finding the most similar book pairs, sorted by hitrate(similarity)</p>
<pre><code>for book in book_filenames:
    most_similar = model_doc.docvecs.most_similar(book)[0][0]
    print("{} - {}".format(book, most_similar))
</code></pre>
<p>Result list below shows that similarity is found between books from same author, but also of same genre.
Folklore tales are paired, so are Gullivers travels and Robinson Crusoe.</p>
<pre><code>Books\A_Dolls_house_Ibsen.rtf - Books\Hedda_Gabler_Ibsen .rtf
Books\Alice_in_Wonderland.rtf - Books\Norwegian_tales_Asbjornsen_Moe.rtf
Books\All_around_the_moon_Verne.rtf - Books\The_secret_of_the_island_verne.rtf
Books\An_archtartic_mystery_verne.rtf - Books\The_secret_of_the_island_verne.rtf
Books\Anthem_Rand.rtf - Books\The_jungle_book_Kipling.rtf
Books\Around_the_world_in_80_days_Verne.rtf - Books\The_secret_of_the_island_verne.rtf
Books\Don_Quixote.rtf - Books\Iliad_Homer.rtf
Books\Fairytales_H_C_Andersen.rtf - Books\Norwegian_tales_Asbjornsen_Moe.rtf
Books\Ghosts_Ibsen.rtf - Books\LIttle_Eyolf_Ibsen.rtf
Books\Great_Expectations_by_Charles_Dickens.rtf - Books\Tale_of_two_cities_dickens.rtf
Books\Gullivers_Travels_Swift.rtf - Books\Robinson_Crusoe _ Defoe.rtf
</code></pre>
<p>WHen testing on specific books, I use the 'most_similar:</p>
<pre><code> model_doc.docvecs.most_similar('Books\Tarzan_of_the_apes.rtf')
</code></pre>
<p>This returns the other Tarzan books, folowed by adventure books for adolescents; quite a good result.</p>
<pre><code>('Books\\The_beasts_of_Tarzan.rtf', 0.7585173845291138),
 ('Books\\Jungle_tales_of_Tarzan.rtf', 0.7063543796539307),
 ('Books\\The_return_of_tarzan.rtf', 0.4879530370235443),
 ('Books\\Tarzan_the_terrible.rtf', 0.4041978716850281),
 ('Books\\An_archtartic_mystery_verne.rtf', 0.319263219833374),
 ('Books\\The_secret_of_the_island_verne.rtf', 0.3165384531021118),
 ('Books\\The_jungle_book_Kipling.rtf', 0.31132781505584717),
 ('Books\\Peter_Pan.rtf', 0.30049124360084534),
 ('Books\\Alice_in_Wonderland.rtf', 0.28424006700515747),
 ('Books\\Robinson_Crusoe _ Defoe.rtf', 0.2718788981437683)

model_doc.docvecs.most_similar(10) (Book 10 = Gullivers Travels by Defoe)

 ('Books\\Robinson_Crusoe _ Defoe.rtf', 0.47113150358200073),
 ('Books\\Pride_and_Prejudices .rtf', 0.3537435233592987),
 ('Books\\Meditations_Aurelius.rtf', 0.3536701798439026),
 ('Books\\Fairytales_H_C_Andersen.rtf', 0.25862976908683777),
 ('Books\\The_secret_of_the_island_verne.rtf', 0.25140485167503357)
</code></pre>
<p>Tsne dimension reduction was done for this vectors as well, and result inspected visually (this is in the Notebook)</p>
<pre><code>all_doc_vectors_matrix = model_doc.wv.vectors
all_doc_vectors_matrix_2 = tsne.fit_transform(all_doc_vectors_matrix)
</code></pre>
<h3><a id="user-content-3-topic-extraction-with-doc2bow-and-lda" class="anchor" aria-hidden="true" href="#3-topic-extraction-with-doc2bow-and-lda"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3) Topic extraction with doc2bow and LDA</h3>
<p>For topic extraction I chose to use a third part of Gensim; doc2bow - using a bag-of-word method (bow: bag-of-words)
I also used another algorithm; LDA; often used for nlp. LDA: Latent Dirichlet Allocation. LDA is also from the Gensim package.</p>
<p>Create a Dictionary from the data, then a bag of words.<br>
About LDA : <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="nofollow">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></p>
<p>Using Alice in Wonderland as example, these are the steps for Topic Extraction, starting with the alice_fin_sent version of preprocessed corpus, as described earlier. For topic extraction, filler words must be removed. A Lda model is build using the dictionary-based matrix.</p>
<pre><code>dictionary_alice = corpora.Dictionary(alice_fin_sent)

doc_term_matrix_alice = [dictionary_alice.doc2bow(doc) for doc in alice_fin_sent]

Lda = gensim.models.ldamodel.LdaModel

ldamodel_alice = Lda(doc_term_matrix_alice, num_topics=3, id2word = dictionary_alice, passes=750)
</code></pre>
<p>Results from this can be seen when we print the topics from the LDA algorithm</p>
<pre><code>alice_topics = ldamodel_alice.print_topics(num_words=5)
for topic in alice_topics:
    print(topic)
</code></pre>
<p>Presenting the proposed 3 topics using 50 passes:</p>
<pre><code>(0, '0.014*"one" + 0.012*"duchess" + 0.009*"alice" + 0.008*"first" + 0.008*"little"')
(1, '0.036*"alice" + 0.018*"little" + 0.016*"said" + 0.012*"rabbit" + 0.009*"could"')
(2, '0.056*"said" + 0.037*"alice" + 0.012*"king" + 0.012*"know" + 0.009*"like"')
</code></pre>
<p>Setting to 250 passes:</p>
<pre><code>(0, '0.031*"alice" + 0.021*"said" + 0.010*"mouse" + 0.009*"thought" + 0.009*"go"')
(1, '0.041*"alice" + 0.037*"said" + 0.010*"rabbit" + 0.007*"came" + 0.007*"moment"')
(2, '0.017*"one" + 0.017*"little" + 0.013*"said" + 0.013*"alice" + 0.012*"way"')
</code></pre>
<p>Setting to 750 passes:</p>
<pre><code>0, '0.066*"said" + 0.042*"alice" + 0.010*"duchess" + 0.009*"oh" + 0.009*"think"')
(1, '0.014*"little" + 0.013*"know" + 0.010*"alice" + 0.009*"said" + 0.009*"queen"')
(2, '0.031*"alice" + 0.015*"rabbit" + 0.011*"little" + 0.009*"white" + 0.008*"one"')
</code></pre>
<p>The topic extraction must be performed for each book, and on a sentence level. As shown, I varied the no: of passes, and also the no of topics and num_words. It is not obvious that the results increase with more passes.</p>
<pre><code>Is "said+alice+duchess+oh+think' really better than 'one+duchess+alice+first+little' ?
</code></pre>
<p>I can see that Proposed Topics are similar to the book's frequent words, but not 100% identical.</p>
<p>The same topic extraction steps were done for 5 other books, with similar results.
As  a pattern, topics seem to include the books protagonist, the second most important character (if any) and some typical words</p>
<h3><a id="user-content-refinement" class="anchor" aria-hidden="true" href="#refinement"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Refinement</h3>
<p>NLP enjoys a wide variety of resources for prepping and transforming data, Nevertheless, there is room for coding and adjusting before, during and after package usage.</p>
<p>Several of the improvement measures taken are mentioned above, a recap is as following;</p>
<p>Preprocess : Data gathering and cleaning</p>
<pre><code>Legal clausuls at the end of the books were removed completely as I saw they introduced 'noise' in the text
Some sentences in the book's beginnings were removed for same reasons; Internet Urls, email addresses etc. 
Tokenizers on sentence and word level were tested and compared; I chose the one that kept full original words
</code></pre>
<ol>
<li>
<p>Word analysis</p>
<p>Fdist results from raw_corpus were poor, I changed the input to sentences with both cleaning of characters and stopwords removed</p>
<pre><code>Word2Vec model parameters:
    num_features = 300 (increased from 200 )
    min_word_count = 20 (inreased from 10 to 20)
    num_workers = multiprocessing.cpu_count() (unchanged)
    context_size = 10 (increased from 5 to 10)
    downsampling = 1e-4 (unchanged)
    seed = 9 (unchanged)
</code></pre>
</li>
</ol>
<p>TSNE parameters were tweaked similarly; aiming a balance between results and runtime</p>
<ol start="2">
<li>Book analysis</li>
</ol>
<p>The book analysis is similar to the word analysis method-wise. I did tryouts with parameters for the model and the tsne dimension reduction. On book-level, the model seems less sensitive for parameter tuning, probably because we operate on a book-level where similarity will be more obvious, the most typical result being similarity by author, followed by genres, picking out adventure stories for adolescents.</p>
<ol start="3">
<li>Topic extraction</li>
</ol>
<p>The final task turned out to be quite straightforward to perform when I had found the recommended method from books and articles.</p>
<p>LDA parameters to investigate are passes, no of topics and no of words. Increasing number of passes did improve the results slightly.</p>
<p>The method is compact when all data is preprocessed and the steps prepared code-wise. I chose to write small code packages for a 'pipeline' and tested this on several books. The results are of varying quality, but confirm the idea of being able to extract topics automatically from larger texts, such as my books.</p>
<p>One of the useful articles about the topic is the following, by Susan Li :
<a href="https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21" rel="nofollow">https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21</a></p>
<h3><a id="user-content-learnings-and-challenges" class="anchor" aria-hidden="true" href="#learnings-and-challenges"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Learnings and challenges</h3>
<h4><a id="user-content-preprocess-learnings" class="anchor" aria-hidden="true" href="#preprocess-learnings"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Preprocess learnings</h4>
<p>My main learning from the preprocessing is the importance of understanding how the output from this processing will impact the next step. Cleaning text is not equal to cleaning numerical and categorical values, and there are more choices to be made.</p>
<p>The format of the text preprocessing output must match the expected format for the vectorization.
Providing a text corpus when tokens are expected will return analysis of single characters, not 'words'.
For most purposes the expetced format is of clean tokens on a sentence level.</p>
<pre><code>Example: for one test I got a 'vocabulary' of 74; it turned out to be the 74 characters available on my keyboard
Example 2: I quite often got the error message 'expected format 1, got format 2'.  
</code></pre>
<p>The use of tokenizers and other cleaning code options is crucial to get sensible results. Even if I read this on beforehand, it was suprising to see how large a part of the text could be removed/cleaned and still keep the essence of the content.
This does not imply that all cleaning tools should always be applied. If we want to keep track of all persons in a novel, it is probably not wise to convert all words to lowercase. And - not all analysises make sense, as for most frequent sentences</p>
<pre><code>Example: My first attempts to find most frequent words were useless before I removed thrash characters and stopwords
Example: Trying to find most frequent sentences returned only 'Names:'. Why? These are from plays; 'Nora:', Hedda:' etc 
</code></pre>
<p>When getting to the vectorization and model usage I encountered more challenges. Again, the input had to be a correct format and the level aligned with the model. Word2Vec is used on the whole corpus, but Doc2Vec is applied on a document level.</p>
<p>My first attemps on the Topic Extractions Doc2Vec failed, first because of format mismatch (again!), then on performance as I though I could perform this on the full collection, this being tagged by individual books. This was not a success, the job went on for 5-6 hours and may never have completed. I chose to perform the training on document (book) level instead, which went perfectly fine.</p>
<p>Some of the code for use of models is written based on similar examples found in books, forums etc. As software is rapidly evolving, I often experience that syntax is outdated and must be changed. This is not a big challenge, but must be handled.</p>
<pre><code>Example: word2vec syntax had to be updated slightly from my first attempts to avoud warning when run
</code></pre>
<p>There are many useful library resources that makes nlp easier than hand-coding. I did some tryouts with vectors to analyse the word-based corpus, spending hours with vector handling, sorting and formatting the results - to get exactly the same result as from the compact FreqDist() function from gensim. (This is left in the Notebook as an 'effort documentation').</p>
<pre><code>Example: There is an easy way and a hard way to find frequent words, choose one for learning, one for work
</code></pre>
<p>The most challenging in this project was not in the coding in itself, but navigating among the different options and understanding the connection bethween the initial data and the results I wanted to achieve. The choices of methods for analysis are many, and I realize I have only schratched the supface of this topic.</p>
<h2><a id="user-content-iv-results" class="anchor" aria-hidden="true" href="#iv-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>IV. Results</h2>
<p>A brief summary of the results for the three main tasks :</p>
<h4><a id="user-content-word-analysis-results" class="anchor" aria-hidden="true" href="#word-analysis-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Word analysis results</h4>
<pre><code>Statistics vere produced for corpus and books, on different levels
Corpus and vocabulary could be investigated, with a clear visual of vocabulary in a plotted wordcloud
I could zoom in on chosen words, see the vicinity words
Identifying most frequent words was done
Similarity words were delivered for any chosen word
Word pairs were produced on a satisfying level
</code></pre>
<h4><a id="user-content-book-similarity-1" class="anchor" aria-hidden="true" href="#book-similarity-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Book similarity</h4>
<pre><code>Good results after parameter tweaks. Obvious similarity between books by the same author, also similarity caused by plot and genre.
Visuals in form of a wordcloud as for the word analysis 
</code></pre>
<h4><a id="user-content-topic-extraction-1" class="anchor" aria-hidden="true" href="#topic-extraction-1"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Topic extraction</h4>
<pre><code>Acceptable good results. 
Verification of result, in addition to scoring figures done by book knowledge and a generous interpretation of the topic words.   
</code></pre>
<p>Test of unknown book
Finally, I found a completely unknown book on Gutenberg and performed Topic extraction on the text</p>
<pre><code>The-Chemistry-of-Food-and-Nutrition.txt
</code></pre>
<p>The results from this old book about food and chemistry strikes me by being suprisingly relevant for today's view on food and health.</p>
<pre><code>'0.017*"food" + 0.016*"protein" + 0.012*"quantity" + 0.011*"matter"'
'0.007*"salt" + 0.007*"one" + 0.006*"food" + 0.005*"meat"'
</code></pre>
<h3><a id="user-content-extra--test-of-a-contemporary-document" class="anchor" aria-hidden="true" href="#extra--test-of-a-contemporary-document"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Extra : Test of a contemporary document</h3>
<p>For fun and curiosity, I tested the topic extraction on a random modern document; "Google Chrome Terms of Service"</p>
<p>Spending 5 minutes downloading the file, running it through the code, gave this topic extraction:</p>
<pre><code>(0, '0.031*"terms" + 0.030*"adobe" + 0.020*"software" + 0.020*"use" + 0.020*"sublicensee" + 0.019*"shall" + 0.017*"may"')
(1, '0.030*"sublicensee" + 0.028*"terms" + 0.021*"google" + 0.020*"adobe" + 0.018*"rights" + 0.013*"additional" + 0.012*"may"')
(2, '0.035*"adobe" + 0.028*"software" + 0.018*"video" + 0.018*"code" + 0.017*"content" + 0.014*"may" + 0.013*"avc"')
(3, '0.071*"google" + 0.032*"terms" + 0.027*"services" + 0.020*"agreement" + 0.018*"adobe" + 0.018*"chrome" + 0.014*"content"')
(4, '0.046*"google" + 0.038*"services" + 0.023*"may" + 0.021*"use" + 0.019*"software" + 0.017*"adobe" + 0.017*"sublicensee"')
</code></pre>
<p>It is not a trimmed executive summary, but it indicates the content, ie by the last topic suggestion</p>
<pre><code>"google-services-may-use-software-adobe-sublicensee'
</code></pre>
<h3><a id="user-content-additional-model-evaluation" class="anchor" aria-hidden="true" href="#additional-model-evaluation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Additional Model Evaluation</h3>
<h4><a id="user-content-model-sensitivity-and-robustness" class="anchor" aria-hidden="true" href="#model-sensitivity-and-robustness"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model sensitivity and robustness</h4>
<p>Analysing data is heavily dependent on thorough data cleaning and preparation, even more so than many other projects I have done. The maths may be right but the output of little value if context and text characteristics are not handled properly. Tuning the models improved the results somewhat, but not with any dramatic, threshold effects. The resulting robustness seems to vary beween the topics;</p>
<pre><code>Word analysis: Data must be clean and trimmed with good text insight. Parameters in model important for results.  

Document similarity : The least sensitive topic, the results are fair on all tries, parameters have moderate effect

Topic extraction : The results were clearly affected by model parameters, but more difficult to asses quality in results. 
This may sound strange; the suggested topic word change, but the value of topic description is difficult to measure. 
</code></pre>
<p>In short; the quality of the results seem to be more dependent on data quality and method choice rather than model parameters.</p>
<h3><a id="user-content-comments" class="anchor" aria-hidden="true" href="#comments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comments</h3>
<p>Model usability</p>
<p>in this Kaggle post below (chapter 2 and 3) -  and in other similar articles, different options for further work on the word vectors, beyond the bag-of_words (bow), are discussed. Is seems to be a common consensus that the 'bow' method is robust and gives consistently good results compared to other methods.</p>
<p><a href="https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words" rel="nofollow">https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words</a></p>
<h3><a id="user-content-justification" class="anchor" aria-hidden="true" href="#justification"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Justification</h3>
<p>Benchmarking these results with manual handling of text is not easily measured, but I'll provide my reflections.</p>
<p>The important question I started with was; can we compare documents and extract essence in an effective way?</p>
<p>41 books in the mini-library may sound like a lot of words, but in this dicipline it forms a meagre corpus, yet proved enough to do the project and get interesting results.</p>
<p>Comparing documents by applying code to literal pieces of text as the library represents is interesting, but only partly representative for my real-life challenge. The documents in scope for implementation in DNB will be more specific and probably easier to distinguish  by context vocabulary, aligning Trade Finance documents and separating those from ie Loan Agreements.</p>
<h2><a id="user-content-v-conclusion-and-reflection" class="anchor" aria-hidden="true" href="#v-conclusion-and-reflection"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>V. Conclusion and Reflection</h2>
<h3><a id="user-content-short-summary-of-the-steps-taken" class="anchor" aria-hidden="true" href="#short-summary-of-the-steps-taken"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Short summary of the steps taken</h3>
<pre><code>Finding the best case and relating to my personal interests, defining the case in the project proposal

Choosing the best 'mini-library' according to my criteria

Reading the book on text analysis for more knowledge in the field

First assessments of the quality of the texts, doing some initial corrections

A deep-dive into the world of tokenization on word and sentence level - and other cleaning alternatives

Investigating options for word and book analysis; choosing among features in gensim, nltk and scikitlearn

Performing FreqDist on corpus after tokenization

Building the first models in word2vec and doc2vec, partially in parallell

Assessing results, making visuals

Starting the topic extraction part, evaluating whether to to TFidf or not

Building the neccessary 'pipeline' for the steps in the lda-based topic extraction

For all 'lanes' - iterations, testing more books, comparing, improving.

Finally, introducing an 'unknown' book and a new document for test purposes 

Wrapup and report.
</code></pre>
<h3><a id="user-content-visuals-illustrating-the-results" class="anchor" aria-hidden="true" href="#visuals-illustrating-the-results"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Visuals illustrating the results</h3>
<p>There are several visuals included in the report for word frequencies, and word vector distributions.</p>
<p>For fun - but also for giving a symbol of nlp tasks - I have created a wordcloud for Vernes '80 days around the world'. It confirms the simple observation mentioned above and the very core of this project; in books the most frequent words seem to be the protagonist (Fix), the word 'said' (dialogue intensive texts) and then a very large number of stopwords and 'data noise'. What we like to think as the 'essence' of books and other texts is quite well hidden in the mass of words and takes elaborate digging and interpretation to understand.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/42574791/49338680-09985080-f625-11e8-9316-4475342607b1.png"><img src="https://user-images.githubusercontent.com/42574791/49338680-09985080-f625-11e8-9316-4475342607b1.png" alt="image" style="max-width:100%;"></a></p>
<p>Diving into NLP has been challenging yet satisfying as this is not a redo of projects but new skills and learnings.</p>
<p>This is not a straight-forward technique but an area where any project will be a series of crossroads where choices must be made of methods, algorithms and so on. Iterations is neccessary, and as metioned earlier there will be separate lanes for the individual questions.</p>
<p>Topic extraction is the problem I am most eager to try out internally. This is the part of the project where there is the most room for improvement, if looking at the results. Yes, the topics suggested are ok, but the precision and presentetation can be better.</p>
<p>Alltogether, I find the results from a compact Capstone project, perfomed with a very basic understanding of the topic, to be promising. The goal of building insight and understanding has abslutely met my expectation from a personal view.</p>
<h2><a id="user-content-improvement" class="anchor" aria-hidden="true" href="#improvement"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Improvement</h2>
<p>My project did not include establishing a user interface for input of random documents, nor a prepped output in the form of screenbased result presentation or printable report. For my real-life problem this will have to be made, but not neccessarily a glossy user interfase, as this is for internal use in back office environments, nor for customer usage externally.</p>
<p>Topic extraction need to be investigated and tuned more, to find the optimal parameters for a given corpus.</p>
<p>Overall, improvements wil have to be initiated by learning more; reading and trying out more options.</p>
</article>
  </div>

    </div>

  

  <details class="details-reset details-overlay details-overlay-dark">
    <summary data-hotkey="l" aria-label="Jump to line"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast linejump" aria-label="Jump to line">
      <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-jump-to-line-form Box-body d-flex" action="" accept-charset="UTF-8" method="get"><input name="utf8" type="hidden" value="&#x2713;" />
        <input class="form-control flex-auto mr-3 linejump-input js-jump-to-line-field" type="text" placeholder="Jump to line&hellip;" aria-label="Jump to line" autofocus>
        <button type="submit" class="btn" data-close-dialog>Go</button>
</form>    </details-dialog>
  </details>


  </div>
  <div class="modal-backdrop js-touch-events"></div>
</div>

    </div>
  </div>

  </div>

        
<div class="footer container-lg px-3" role="contentinfo">
  <div class="position-relative d-flex flex-justify-between pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <ul class="list-style-none d-flex flex-wrap ">
      <li class="mr-3">&copy; 2018 <span title="0.25114s from unicorn-75d7b69cf8-sd4jz">GitHub</span>, Inc.</li>
        <li class="mr-3"><a data-ga-click="Footer, go to terms, text:terms" href="https://github.com/site/terms">Terms</a></li>
        <li class="mr-3"><a data-ga-click="Footer, go to privacy, text:privacy" href="https://github.com/site/privacy">Privacy</a></li>
        <li class="mr-3"><a href="/security" data-ga-click="Footer, go to security, text:security">Security</a></li>
        <li class="mr-3"><a href="https://status.github.com/" data-ga-click="Footer, go to status, text:status">Status</a></li>
        <li><a data-ga-click="Footer, go to help, text:help" href="https://help.github.com">Help</a></li>
    </ul>

    <a aria-label="Homepage" title="GitHub" class="footer-octicon mr-lg-4" href="https://github.com">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
</a>
   <ul class="list-style-none d-flex flex-wrap ">
        <li class="mr-3"><a data-ga-click="Footer, go to contact, text:contact" href="https://github.com/contact">Contact GitHub</a></li>
        <li class="mr-3"><a href="https://github.com/pricing" data-ga-click="Footer, go to Pricing, text:Pricing">Pricing</a></li>
      <li class="mr-3"><a href="https://developer.github.com" data-ga-click="Footer, go to api, text:api">API</a></li>
      <li class="mr-3"><a href="https://training.github.com" data-ga-click="Footer, go to training, text:training">Training</a></li>
        <li class="mr-3"><a href="https://blog.github.com" data-ga-click="Footer, go to blog, text:blog">Blog</a></li>
        <li><a data-ga-click="Footer, go to about, text:about" href="https://github.com/about">About</a></li>

    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>
</div>



  <div id="ajax-error-message" class="ajax-error-message flash flash-error">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
    </button>
    You can’t perform that action at this time.
  </div>


    <script crossorigin="anonymous" integrity="sha512-WnyO4VoIUwWWQOmFLjYf4UGg/c1z9VlaLN8IMuiI3uMhhl6rejyThRdLPDyePeUPW6N+38OoBMs6AkqcvWALtA==" type="application/javascript" src="https://assets-cdn.github.com/assets/compat-b66b5d97b4442a01f057c74b091c4368.js"></script>
    <script crossorigin="anonymous" integrity="sha512-glXpCrOqxNGPFRZ0xGRWMgpYC4Xn/c/CL82vk2eeRostCZHF7Px1wBjxv6Wohcq9+xhM1Lt7DvrkWcSvF3j28w==" type="application/javascript" src="https://assets-cdn.github.com/assets/frameworks-7fcedd2a70cd3dde438cbad4c2d08128.js"></script>
    
    <script crossorigin="anonymous" async="async" integrity="sha512-NnkR3Qo+4D5ma5koFlDfAf9bKKHnuAC5BlrEUdKfDKZo46qS4hjt+C4Uw5KtLsTBASZoMneayDVNRxQRucQXqw==" type="application/javascript" src="https://assets-cdn.github.com/assets/github-927061254931b1e409112c1baeed3a26.js"></script>
    
    
    
  <div class="js-stale-session-flash stale-session-flash flash flash-warn flash-banner d-none">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"/></svg>
    <span class="signed-in-tab-flash">You signed in with another tab or window. <a href="">Reload</a> to refresh your session.</span>
    <span class="signed-out-tab-flash">You signed out in another tab or window. <a href="">Reload</a> to refresh your session.</span>
  </div>
  <div class="facebox" id="facebox" style="display:none;">
  <div class="facebox-popup">
    <div class="facebox-content" role="dialog" aria-labelledby="facebox-header" aria-describedby="facebox-description">
    </div>
    <button type="button" class="facebox-close js-facebox-close" aria-label="Close modal">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
    </button>
  </div>
</div>

  <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default text-gray-dark" open>
    <summary aria-haspopup="dialog" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
        <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"/></svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

  <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box box-shadow-large" style="width:360px;">
  </div>
</div>

<div id="hovercard-aria-description" class="sr-only">
  Press h to open a hovercard with more details.
</div>


  </body>
</html>

